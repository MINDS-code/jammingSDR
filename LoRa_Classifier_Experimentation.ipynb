{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhSxiTx4cGTC"
      },
      "source": [
        "#LoRa Signal Classifier\n",
        "With Experimentation Data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OXJVtOmkd9Eu"
      },
      "source": [
        "Imports and Constants\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieZGPvgPd-NN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os, re\n",
        "\n",
        "PATH_DATASETS = ''\n",
        "PATH_MODELS = ''\n",
        "PATH_RESULTS = ''\n",
        "NUM_CLASSES = 18\n",
        "MODEL_INPUT_SHAPE = (512, 1)\n",
        "MODEL_OUTPUT_SHAPE = (NUM_CLASSES,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IEsfrUi-4TP"
      },
      "source": [
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1rkSk5o-7SP"
      },
      "outputs": [],
      "source": [
        "def one_hot_encode(config, num_classes= NUM_CLASSES):\n",
        "    # Example: one_hot_encode(config=1, num_classes=18) -> np.array([0, 1, 0, ..., 0, 0, 0])\n",
        "    if config < 0 or config >= num_classes:\n",
        "        raise ValueError('config must be between 0 and num_classes-1')\n",
        "    return np.eye(num_classes)[config]\n",
        "\n",
        "def parse_file_name(file_name):\n",
        "  # Example: parse_file_name('IF_Sample_P04_C01_N0001.npy') -> {'power': '4', 'config': '1', 'count': '1', 'label': one_hot_encode(config=1, num_classes=18)}\n",
        "  pattern = re.compile(r'IF_Sample_P(?P<power>\\d+)_C(?P<config>\\d+)_N(?P<count>\\d+).npy')\n",
        "  match = pattern.match(file_name)\n",
        "  if match:\n",
        "    return {\n",
        "      'power': int(match.group('power')),\n",
        "      'config': int(match.group('config')),\n",
        "      'count': int(match.group('count')),\n",
        "      'label': one_hot_encode(config=int(match.group('config')))\n",
        "    }\n",
        "  else:\n",
        "      raise ValueError('file_name does not match the pattern')\n",
        "\n",
        "def data_generator(path_dataset, file_list, batch_size):\n",
        "  while True:\n",
        "    batch_data, batch_labels = [], []\n",
        "    for file_name in file_list:\n",
        "      sample = np.load(os.path.join(path_dataset, file_name))\n",
        "      label = parse_file_name(file_name)['label']\n",
        "      if sample.shape == MODEL_INPUT_SHAPE and label.shape == MODEL_OUTPUT_SHAPE:\n",
        "        batch_data.append(sample)\n",
        "        batch_labels.append(label)\n",
        "      if len(batch_data) == batch_size:\n",
        "        yield np.array(batch_data), np.array(batch_labels)\n",
        "        batch_data, batch_labels = [], []\n",
        "    if batch_data:\n",
        "      yield np.array(batch_data), np.array(batch_labels)\n",
        "\n",
        "def group_files_to_dict(files, field):\n",
        "  groups = {}\n",
        "  for file_name in files:\n",
        "    value = parse_file_name(file_name)[field]\n",
        "    if value not in groups:\n",
        "        groups[value] = []\n",
        "    groups[value].append(file_name)\n",
        "  return groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQsumPtmixTl"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def split_data_uniform(data, val_split, max_n, shuffle=True):\n",
        "    data_dict = {}\n",
        "    for filename in data:\n",
        "        key = \"_\".join(filename.split(\"_\")[2:4])  # Get 'Pxx_Cxx' from the filename\n",
        "        if key not in data_dict:\n",
        "            data_dict[key] = []\n",
        "        data_dict[key].append(filename)\n",
        "\n",
        "    # Now we perform train-test split for each 'Pxx_Cxx' group\n",
        "    train_files, test_files = [], []\n",
        "    split_index = int((max_n + 1) * (1 - val_split))  # Split index based on the split ratio\n",
        "    for key in data_dict:\n",
        "        # Shuffle and split the files\n",
        "        if shuffle:\n",
        "            random.shuffle(data_dict[key])\n",
        "\n",
        "        test_files.extend(data_dict[key][split_index:])\n",
        "        if key[:3] == 'P00' or key[:3] == 'P02':\n",
        "          continue\n",
        "        train_files.extend(data_dict[key][:split_index])\n",
        "\n",
        "    return train_files, test_files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV2QhSzSfOJl"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jBD0vrBfO6y"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "  model = Sequential([\n",
        "      Flatten(input_shape=MODEL_INPUT_SHAPE),\n",
        "      Dense(16, activation='tanh'),\n",
        "      Dense(16, activation='tanh'),\n",
        "      Dropout(0.5),\n",
        "      Dense(NUM_CLASSES, activation='softmax')\n",
        "  ])\n",
        "  model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def load_model(model_name):\n",
        "  path_model = os.path.join(PATH_MODELS, model_name)\n",
        "  return tf.keras.models.load_model(path_model)\n",
        "\n",
        "def save_model(model, model_name):\n",
        "  path_model = os.path.join(PATH_MODELS, model_name)\n",
        "  model.save(path_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ey-0jTxU40NC"
      },
      "outputs": [],
      "source": [
        "def train_model(model, path_dataset, train_data, validation_data, epochs,\n",
        "                batch_size):\n",
        "  num_samples = len(train_data)\n",
        "  steps_per_epoch = (num_samples // batch_size) - 1\n",
        "  train_data_gen = data_generator(path_dataset, train_data, batch_size)\n",
        "\n",
        "  num_val_samples = len(validation_data)\n",
        "  validation_steps = (num_val_samples // batch_size)-1\n",
        "  val_data_gen = data_generator(path_dataset, validation_data, batch_size)\n",
        "\n",
        "  model.fit(\n",
        "      train_data_gen,\n",
        "      epochs=epochs,\n",
        "      steps_per_epoch=steps_per_epoch,\n",
        "      validation_data=val_data_gen,\n",
        "      validation_steps=validation_steps,\n",
        "      verbose=1)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8tHN0H9UFAl"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, path_dataset, test_files, validation_steps):\n",
        "    test_data = group_files_to_dict(test_files, field= 'power')\n",
        "    result = {}\n",
        "    for power_level in test_data.keys():\n",
        "        values = []\n",
        "        data_gen = data_generator(path_dataset, test_data[power_level], batch_size=16)\n",
        "        for _ in range(validation_steps):\n",
        "            samples, labels = next(data_gen)\n",
        "            predictions = model.predict(samples)\n",
        "            values += [np.array(v) for v in zip(predictions, labels)]\n",
        "        result[power_level] = np.array(values)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl3qmIl4za-2"
      },
      "source": [
        "Experimentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEFr-AC00kl0"
      },
      "outputs": [],
      "source": [
        "def Monte_Carlo_Cross_Validation(path_dataset, num_iterations= 15,\n",
        "                                 power_levels= list(range(0, 21, 4)),\n",
        "                                 val_split= 0.2, epochs= 10,\n",
        "                                 batch_size= 16):\n",
        "  data = [entry.name\n",
        "          for entry in os.scandir(path_dataset)\n",
        "          if entry.is_file() and entry.name.endswith('.npy')\n",
        "          and entry.name.startswith('IF_Sample')]\n",
        "  # Add code to filter data based on power_levels.\n",
        "  results = []\n",
        "  for i in range(num_iterations):\n",
        "    print(f'Monte Carlo CV iteration {i+1}/{num_iterations}')\n",
        "    train_files, validation_files = split_data_uniform(data, val_split, 50)\n",
        "    model = create_model()\n",
        "    model = train_model(model, path_dataset, train_files, validation_files,\n",
        "                        epochs, batch_size)\n",
        "    num_val_samples = len(validation_files)\n",
        "    validation_steps = (num_val_samples // batch_size)-1\n",
        "\n",
        "    result = evaluate_model(model, path_dataset, validation_files, validation_steps)\n",
        "    results.append(result)\n",
        "    np.save(os.path.join(PATH_RESULTS, f'result_{i+1}.npy'), np.array(results))\n",
        "  return np.array(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPltFFUFzeN_",
        "outputId": "2f374b38-9880-413f-c30c-48822f8ea31d"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  dataset_name = 'SDR_Dataset2'\n",
        "  num_iterations= 30\n",
        "\n",
        "  path_dataset = os.path.join(PATH_DATASETS, dataset_name)\n",
        "  results = Monte_Carlo_Cross_Validation(path_dataset,\n",
        "                                         num_iterations,\n",
        "                                         epochs= 20,\n",
        "                                         batch_size= 8)\n",
        "  np.save(os.path.join(PATH_RESULTS, 'exp_results_5.npy'), results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

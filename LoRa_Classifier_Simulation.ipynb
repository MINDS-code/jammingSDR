{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TarunRao-K/jammingSDR/blob/main/LoRa_Classifier_Simulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQ245ElZ8QQz"
      },
      "source": [
        "#LoRa Signal Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZB4zuL8dbGV"
      },
      "source": [
        "Imports and constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPTO1pCK7FzU"
      },
      "outputs": [],
      "source": [
        "import os, re, random\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
        "from scipy.signal import spectrogram\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "PATH_DATASETS = '/content/drive/MyDrive/LoRa_Detection'\n",
        "PATH_MODELS = '/content/drive/MyDrive/LoRa_Detection/Models'\n",
        "PATH_PREAMBLE_REFERENCE_IQ = '/content/drive/MyDrive/LoRa_Detection/Signal_Data/Preamble_symbols'\n",
        "PATH_RESULTS = '/content/drive/MyDrive/LoRa_Detection/Code'\n",
        "\n",
        "F_SAMP = int(1e6)\n",
        "F_MAX = 250000\n",
        "WINDOW_SIZE = 2**7\n",
        "WINDOW_OVERLAP = WINDOW_SIZE // 2\n",
        "\n",
        "BW_VALUES = [125000, 250000, 500000]\n",
        "SF_VALUES = [7, 8, 9, 10, 11, 12]\n",
        "NUM_CLASSES = 18\n",
        "\n",
        "MODEL_INPUT_SHAPE = (1024, 1)\n",
        "MODEL_OUTPUT_SHAPE = (NUM_CLASSES,)\n",
        "\n",
        "SNR_MIN = -20\n",
        "SNR_MAX = 20\n",
        "SNR_STEP = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf84vXVMZtj1"
      },
      "source": [
        "Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riZjdIJRZvoB"
      },
      "outputs": [],
      "source": [
        "def encode_label(config):\n",
        "  one_hot_label = np.zeros(NUM_CLASSES)\n",
        "  one_hot_label[config] = 1\n",
        "  return one_hot_label\n",
        "\n",
        "def decode_snr(snr):\n",
        "    # Decode SNR from file name (sign: 0/1, magnitude: 0.00-99.99)\n",
        "    snr_sign, snr_mag = divmod(int(snr), 10000)\n",
        "    snr = (-1)*(snr_mag/100) if snr_sign else snr_mag/100\n",
        "    return snr\n",
        "\n",
        "def parse_file_name(file_path):\n",
        "    file_name = os.path.basename(file_path)\n",
        "    match = re.search(r'SNR(\\d{5})D(\\d{2})C(\\d{2})T(\\d{2}).npy', file_name)\n",
        "    if match:\n",
        "        snr = decode_snr(match.group(1))\n",
        "        dist = int(match.group(2))\n",
        "        config = int(match.group(3))\n",
        "        index = int(match.group(4))\n",
        "        return snr, dist, config, index\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def filter_files(dataset_name, filter_snr=None, filter_config=None, filter_count=50, shuffle=True):\n",
        "    files_list = []\n",
        "    path_dataset = os.path.join(PATH_DATASETS, dataset_name)\n",
        "    all_files = os.listdir(path_dataset)\n",
        "    for file_name in all_files:\n",
        "        snr, _, config, index = parse_file_name(file_name)       \n",
        "        if filter_snr is not None:\n",
        "            if int(snr) not in filter_snr:\n",
        "                continue\n",
        "        if filter_config is not None:\n",
        "            if config not in filter_config:\n",
        "                continue\n",
        "        if index > filter_count:\n",
        "          continue\n",
        "        files_list.append(os.path.join(path_dataset, file_name))\n",
        "    if shuffle:\n",
        "        np.random.shuffle(files_list)\n",
        "    return files_list\n",
        "\n",
        "def read_iq_file(filepath):\n",
        "    with open(filepath, \"rb\") as f:\n",
        "        iq_samples = np.fromfile(f, dtype=np.complex64)\n",
        "    return iq_samples\n",
        "\n",
        "def read_IF_sample(file_path):\n",
        "  IF_sample = np.load(file_path)[-MODEL_INPUT_SHAPE[0]:]\n",
        "  return IF_sample\n",
        "\n",
        "def get_reference_iq_samples(bandwidth, spread_factor):\n",
        "  iq_file_path = os.path.join(PATH_PREAMBLE_REFERENCE_IQ,\n",
        "                                  'PR_Ref_BW{}_SF{}.bin'.format(\n",
        "                                      BW_VALUES.index(bandwidth),\n",
        "                                      spread_factor))\n",
        "  return read_iq_file(iq_file_path)\n",
        "\n",
        "def pad_signal(signal_IQ):\n",
        "  signal_len = signal_IQ.shape[0]\n",
        "  input_length= (MODEL_INPUT_SHAPE[0]+1)*WINDOW_OVERLAP\n",
        "\n",
        "  padded_signal = np.zeros(input_length, dtype= np.complex64)\n",
        "  if signal_len <= input_length:\n",
        "    padded_signal[-signal_len:] = signal_IQ\n",
        "  else:\n",
        "    padded_signal = signal_IQ[-input_length:]\n",
        "  return padded_signal\n",
        "\n",
        "def add_gaussian_noise(signal, snr_db):\n",
        "    non_zero_samples = np.nonzero(signal)[0]  \n",
        "    first_non_zero = non_zero_samples[0] if len(non_zero_samples) > 0 else 0\n",
        "    last_non_zero = non_zero_samples[-1] if non_zero_samples[-1] < len(\n",
        "        signal) else len(signal)-1\n",
        "    signal_power = np.mean(np.abs(signal[first_non_zero:last_non_zero+1]) ** 2)\n",
        "    snr = 10 ** (snr_db / 10.0)\n",
        "    noise_power = signal_power / snr\n",
        "    noise = np.sqrt(noise_power / 2) * (np.random.randn(*signal.shape) + 1j * np.random.randn(*signal.shape))    \n",
        "    noisy_signal = signal + noise\n",
        "    return noisy_signal\n",
        "\n",
        "def simulate_iq_samples(bandwidth, spread_factor, snr_db):\n",
        "    IQ_ref = get_reference_iq_samples(bandwidth, spread_factor)\n",
        "    IQ_noisy = add_gaussian_noise(pad_signal(IQ_ref), snr_db)\n",
        "    return IQ_noisy\n",
        "\n",
        "def compute_spectrogram(IQ_samples):\n",
        "  Sxx =  spectrogram(IQ_samples, F_SAMP,window= 'hann',\n",
        "                             nperseg= WINDOW_SIZE,noverlap= WINDOW_OVERLAP,\n",
        "                             return_onesided= False)[2]\n",
        "  Sxx = np.vstack([Sxx[Sxx.shape[0]//2:], Sxx[:Sxx.shape[0]//2]])\n",
        "  Sxx = Sxx[WINDOW_SIZE//4:-WINDOW_SIZE//4, :]\n",
        "  Sxx = (Sxx - np.min(Sxx))/(np.max(Sxx)-np.min(Sxx))\n",
        "  return Sxx\n",
        "\n",
        "def compute_inst_freq(spec_sample):\n",
        "  spec_sample = spec_sample**4\n",
        "  f_bins = np.linspace(-F_MAX, F_MAX, num=spec_sample.shape[0]+1)[:-1]\n",
        "  weighted_sum = np.sum(spec_sample * f_bins[:, np.newaxis], axis=0)\n",
        "  total_power = np.sum(spec_sample, axis=0)\n",
        "  IF_sample = np.clip((weighted_sum / total_power)/F_MAX, -1, 1)[:, np.newaxis]\n",
        "  return IF_sample\n",
        "\n",
        "def simulate_IF_sample(config, snr):\n",
        "  bandwidth, spread_factor = BW_VALUES[config//6], SF_VALUES[config%6]\n",
        "  IQ_samples = simulate_iq_samples(bandwidth, spread_factor, snr)\n",
        "  spec_sample = compute_spectrogram(IQ_samples)\n",
        "  IF_sample = compute_inst_freq(spec_sample)[-MODEL_INPUT_SHAPE[0]:]\n",
        "  return IF_sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE7nQT4iV58h"
      },
      "source": [
        "Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VY2Sec76V7ks"
      },
      "outputs": [],
      "source": [
        "def data_generator(files_list, batch_size):\n",
        "  while True:\n",
        "    batch_data, batch_labels = [], []\n",
        "    batch_sample_count = 0\n",
        "    for file_path in files_list:\n",
        "      sample = read_IF_sample(file_path)\n",
        "      sample = sample * 2 if 'sim' not in file_path.lower() else sample\n",
        "      config = parse_file_name(os.path.basename(file_path))[2]\n",
        "      label = encode_label(config)\n",
        "      if sample.shape==MODEL_INPUT_SHAPE and label.shape==MODEL_OUTPUT_SHAPE:\n",
        "        batch_data.append(sample)\n",
        "        batch_labels.append(label)\n",
        "        batch_sample_count += 1\n",
        "      if batch_sample_count >= batch_size:\n",
        "        indices = np.arange(batch_sample_count)\n",
        "        np.random.shuffle(indices)\n",
        "        yield np.array(batch_data)[indices], np.array(batch_labels)[indices]\n",
        "        batch_data, batch_labels = [], []\n",
        "        batch_sample_count = 0\n",
        "\n",
        "def simulation_data_generator(num_samples_per_case, snr_range, config_range, batch_size):\n",
        "  while True:\n",
        "    batch_data, batch_labels = [], []\n",
        "    batch_sample_count = 0\n",
        "    for _ in range(num_samples_per_case):\n",
        "      for snr in snr_range:\n",
        "        for config in config_range:\n",
        "            sample = simulate_IF_sample(config, snr)\n",
        "            label = encode_label(config)\n",
        "            if sample.shape==MODEL_INPUT_SHAPE and label.shape==MODEL_OUTPUT_SHAPE:\n",
        "              batch_data.append(sample)\n",
        "              batch_labels.append(label)\n",
        "              batch_sample_count += 1\n",
        "            if batch_sample_count >= batch_size:\n",
        "              indices = np.arange(batch_sample_count)\n",
        "              np.random.shuffle(indices)\n",
        "              yield np.array(batch_data)[indices], np.array(batch_labels)[indices]\n",
        "              batch_data, batch_labels = [], []\n",
        "              batch_sample_count = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLxbTtFAX9nt"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKp9yGAfX-bk"
      },
      "outputs": [],
      "source": [
        "def create_model():\n",
        "  model = Sequential([\n",
        "      Flatten(input_shape=MODEL_INPUT_SHAPE),\n",
        "      Dense(16, activation='tanh'),\n",
        "      Dense(16, activation='tanh'),\n",
        "      Dropout(0.5),\n",
        "      Dense(NUM_CLASSES, activation='softmax')\n",
        "  ])\n",
        "  model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def load_model(model_name):\n",
        "  path_model = os.path.join(PATH_MODELS, model_name)\n",
        "  return tf.keras.models.load_model(path_model)\n",
        "\n",
        "def save_model(model, model_name):\n",
        "  path_model = os.path.join(PATH_MODELS, model_name)\n",
        "  model.save(path_model)\n",
        "\n",
        "def train_model_from_files(model, train_data, epochs=5, batch_size=8, validation_data=None):\n",
        "  num_samples = len(train_data)\n",
        "  steps_per_epoch = num_samples // batch_size\n",
        "  train_data_gen = data_generator(train_data, batch_size)\n",
        "  if validation_data is not None:\n",
        "    num_val_samples = len(validation_data)\n",
        "    validation_steps = num_val_samples // batch_size\n",
        "    val_data_gen = data_generator(validation_data, batch_size)\n",
        "  else:\n",
        "    num_samples_per_case = 50\n",
        "    snr_range = range(SNR_MIN, SNR_MAX+1, SNR_STEP)\n",
        "    config_range = range(NUM_CLASSES)\n",
        "    num_val_samples = num_samples_per_case*len(snr_range)*len(config_range)\n",
        "    validation_steps = num_val_samples // batch_size\n",
        "    val_data_gen = simulation_data_generator(num_samples_per_case, snr_range, \n",
        "                                          config_range, batch_size)\n",
        "  model.fit(train_data_gen, epochs=epochs, steps_per_epoch=steps_per_epoch, \n",
        "            validation_data=val_data_gen, validation_steps=validation_steps,\n",
        "            verbose=1)\n",
        "  return model\n",
        "\n",
        "def model_evaluate_sim(model, snr_range = list(range(SNR_MIN, SNR_MAX+1, SNR_STEP)), config_range = list(range(NUM_CLASSES)), num_samples= 32):\n",
        "  sim_results = {}\n",
        "  for snr in snr_range:\n",
        "    samples = np.concatenate([np.array([\n",
        "        simulate_IF_sample(config, snr) for _ in range(num_samples)\n",
        "    ]) for config in config_range])\n",
        "    labels = np.concatenate([np.array(\n",
        "        [encode_label(config)]*num_samples\n",
        "        ) for config in config_range])\n",
        "    predictions = model.predict(samples)\n",
        "    sim_results[snr] = np.stack((predictions, labels), axis = 1)\n",
        "  return sim_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvOxGsUJUX3S"
      },
      "source": [
        "Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x26ZKzIcUZqQ"
      },
      "outputs": [],
      "source": [
        "def MC_CV_sim(filter_snr = list(range(0, 20, 4)), num_samples= 50, \n",
        "              num_iterations=10, num_epochs= 30, val_split=0.2, batch_size=8):\n",
        "  sim_files_list = filter_files('Sim_IF_Dataset2', filter_snr=filter_snr, filter_count=num_samples)\n",
        "  results = []\n",
        "  for i in range(num_iterations):\n",
        "    print(f'MC-CV iteration {i+1}')\n",
        "    random.shuffle(sim_files_list)\n",
        "    split_idx = int(len(sim_files_list)*(1-val_split))\n",
        "    train_files = sim_files_list[:split_idx]\n",
        "    validation_files = sim_files_list[split_idx:]\n",
        "    model_i = create_model()\n",
        "    model_i = train_model_from_files(model_i, train_data= train_files, validation_data= validation_files, epochs=num_epochs, batch_size=batch_size)\n",
        "    results_i = model_evaluate_sim(model_i, snr_range=list(range(-20, 21, 4)), num_samples=num_samples)\n",
        "    results.append(results_i)\n",
        "  print('MC-CV end!')\n",
        "  return list(filter_snr), np.array(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "oKSJ6wii8rCk"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "  snr_range, results = MC_CV_sim(filter_snr= list(range(0, 21, 2)),\n",
        "                                num_samples= 40, num_iterations=15,\n",
        "                                num_epochs= 10, batch_size= 8)\n",
        "  np.save(os.path.join(PATH_RESULTS, 'results_007.npy'), results)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "mount_file_id": "1eIwfJREU_JvXOV8CzpxeyJhl3HaA_qF8",
      "authorship_tag": "ABX9TyNzGSKK30VrP6sq8+BcPXun",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}